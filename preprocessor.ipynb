{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "import torchtext as text\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_sentencizer = spacy.blank(\"en\")\n",
    "nlp_sentencizer.add_pipe(\"sentencizer\")\n",
    "\n",
    "nlp_sentencizer.max_length = 6351665\n",
    "\n",
    "corpus = open('Auguste_Maquet.txt', 'r')\n",
    "ds = corpus.read()\n",
    "ds.replace(\"\\n\", \"\")\n",
    "tokens = nlp_sentencizer(ds)\n",
    "\n",
    "sents = []\n",
    "\n",
    "for sent in tokens.sents:\n",
    "    sents.append(str(sent).replace('\\n', \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(47)\n",
    "random.shuffle(sents)\n",
    "\n",
    "train = sents[:20000]\n",
    "test = sents[20001:30001]\n",
    "\n",
    "train = ''.join(str(train))\n",
    "\n",
    "test = ''.join(str(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        text = text.lower()\n",
    "        text = text.strip('\\n')\n",
    "        text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\n])|(\\w+:\\/\\/\\S+)|^rt|www.+?\", \" \", text)\n",
    "\n",
    "        expanded_text = []\n",
    "        for word in text.split():\n",
    "            expanded_text.append(word) \n",
    "        return expanded_text\n",
    "\n",
    "def build_vocab(text):\n",
    "        vocab = {}\n",
    "\n",
    "        for word in text:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) # add a new type to the vocab\n",
    "        return vocab\n",
    "\n",
    "def process_test_data(test_data, vocab):\n",
    "    process_data = test_data.copy()\n",
    "    for  i, word in enumerate(test_data):\n",
    "        if word not in vocab:\n",
    "            process_data[i] = 'unk' \n",
    "    return process_data\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = clean_text(train)\n",
    "\n",
    "test_data = clean_text(test)\n",
    "\n",
    "vocabulary = build_vocab(train_data)\n",
    "#\n",
    "vocabulary['unk'] = -1\n",
    "\n",
    "test_data_processed = process_test_data(test_data, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
