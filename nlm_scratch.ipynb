{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['bob likes sheep', 'alice is fast', 'cs685 is fun', 'i love lamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given first 2 words of sentence, try to predict third, using fixed window nlm\n",
    "\n",
    "start by tokenizing\n",
    "convert words to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bob': 0, 'likes': 1, 'sheep': 2, 'alice': 3, 'is': 4, 'fast': 5, 'cs685': 6, 'fun': 7, 'i': 8, 'love': 9, 'lamp': 10}\n",
      "[[0, 1, 2], [3, 4, 5], [6, 4, 7], [8, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "vocab = {} # map from word type to index\n",
    "inputs = [] # stores an indexified version of rach sentence\n",
    "for sent in sentences:\n",
    "    sent_idexs = []\n",
    "    sent = sent.split() #tokenize w/ whitespace\n",
    "    for w in sent:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = len(vocab) # add a new type to the vocab\n",
    "        sent_idexs.append(vocab[w])\n",
    "    inputs.append(sent_idexs)\n",
    "\n",
    "print(vocab)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [3, 4],\n",
      "        [6, 4],\n",
      "        [8, 9]]) tensor([ 2,  5,  7, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# two things: 1. convert to LongTensors 2. Define inputs/outputs\n",
    "\n",
    "prefixes = torch.LongTensor([sent[:-1] for sent in inputs ])\n",
    "labels = torch.LongTensor([sent[-1] for sent in inputs])\n",
    "\n",
    "print(prefixes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels tensor([ 2,  5,  7, 10])\n",
      "output  torch.Size([4, 11])\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 0 ,  loss 2.59\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 1 ,  loss 2.27\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 2 ,  loss 1.98\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 3 ,  loss 1.72\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 4 ,  loss 1.47\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 5 ,  loss 1.25\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 6 ,  loss 1.04\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 7 ,  loss 0.86\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 8 ,  loss 0.71\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 9 ,  loss 0.58\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 10 ,  loss 0.48\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 11 ,  loss 0.41\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 12 ,  loss 0.34\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 13 ,  loss 0.29\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 14 ,  loss 0.25\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 15 ,  loss 0.22\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 16 ,  loss 0.19\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 17 ,  loss 0.17\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 18 ,  loss 0.15\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 19 ,  loss 0.14\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 20 ,  loss 0.12\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 21 ,  loss 0.11\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 22 ,  loss 0.10\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 23 ,  loss 0.10\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 24 ,  loss 0.09\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 25 ,  loss 0.08\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 26 ,  loss 0.08\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 27 ,  loss 0.07\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 28 ,  loss 0.07\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 29 ,  loss 0.06\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 30 ,  loss 0.06\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 31 ,  loss 0.06\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 32 ,  loss 0.05\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 33 ,  loss 0.05\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 34 ,  loss 0.05\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 35 ,  loss 0.05\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 36 ,  loss 0.04\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 37 ,  loss 0.04\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 38 ,  loss 0.04\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 39 ,  loss 0.04\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 40 ,  loss 0.04\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 41 ,  loss 0.03\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 42 ,  loss 0.03\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 43 ,  loss 0.03\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 44 ,  loss 0.03\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 45 ,  loss 0.03\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 46 ,  loss 0.03\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 47 ,  loss 0.03\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 48 ,  loss 0.03\n",
      "output  torch.Size([4, 11])\n",
      "logits, labels torch.Size([4, 11]) torch.Size([4])\n",
      "epoch 49 ,  loss 0.03\n"
     ]
    }
   ],
   "source": [
    "# Netowrk\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class NLM(nn.Module):\n",
    "     # two things to do\n",
    "     # 1. init function (initializes all the params of the network)\n",
    "     # 2. forward function (defines forward propogation computation)\n",
    "\n",
    "    def __init__(self, d_embedding, d_hidden, window_size, len_vocab):\n",
    "        super(NLM, self).__init__() # init the base Module Class\n",
    "        self.d_emb = d_embedding\n",
    "        self.embeddings = nn.Embedding(len_vocab, d_embedding)\n",
    "\n",
    "        # concatenated embeddings -> hidden\n",
    "        self.W_hid = nn.Linear(d_embedding*window_size, d_hidden)\n",
    "\n",
    "        # hidden -> output probability distribution over vocab\n",
    "\n",
    "        self.W_out = nn.Linear(d_hidden, len_vocab)\n",
    "\n",
    "    def forward(self, input): # each input will be a batch of prefixes\n",
    "        batch_size, window_size = input.size()\n",
    "        embs = self.embeddings(input) # 4x2x5\n",
    "        # print('emb ', embs.size())\n",
    "        # next concatenate the prefix embeddings together\n",
    "        concat_embs = embs.view(batch_size, window_size* self.d_emb) # 4 x 10\n",
    "        # print('conactenated embs ', concat_embs.size())\n",
    "\n",
    "        #project to hidden space\n",
    "\n",
    "        hidden = self.W_hid(concat_embs) # 4 x 12\n",
    "\n",
    "        # print(hidden.size())\n",
    "\n",
    "        # project to vocabulary space\n",
    "\n",
    "        out = self.W_out(hidden)\n",
    "\n",
    "        print('output ', out.size())\n",
    "\n",
    "        return out # return logits, unnormalized\n",
    "\n",
    "        # probs = nn.functional.softmax(out, dim=1)\n",
    "\n",
    "        #print(probs.size())\n",
    "\n",
    "\n",
    "network = NLM(d_embedding=5, d_hidden=12, window_size=2, len_vocab=len(vocab))\n",
    "print('labels', labels)\n",
    "network(prefixes)\n",
    "        \n",
    "\n",
    "num_epochs = 50 # num passes through training data\n",
    "learning_rate = 0.1 #can modify\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "# adam, adamax\n",
    "optimizer = torch.optim.SGD(params=network.parameters(), lr = learning_rate)\n",
    "\n",
    "# training loop\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    logits = network(prefixes)\n",
    "\n",
    "    print('logits, labels', logits.shape, labels.shape)\n",
    "\n",
    "    loss = loss_fn(logits,labels)\n",
    "\n",
    "\n",
    "    #compute gradient (partial derivativ w.r.t loss)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # update params using gradient descent\n",
    "    optimizer.step()\n",
    "\n",
    "    #zero gradient for next epoch\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print('epoch %d ,  loss %0.2f' %(i,loss) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
